{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Important Library\n\n!pip install sympy\nfrom sympy import *\n","metadata":{"id":"gWg17Plk8bwE","outputId":"61ed482c-44c7-440b-d21a-343a123464af","execution":{"iopub.status.busy":"2022-04-13T18:09:30.351870Z","iopub.execute_input":"2022-04-13T18:09:30.352182Z","iopub.status.idle":"2022-04-13T18:09:43.423734Z","shell.execute_reply.started":"2022-04-13T18:09:30.352125Z","shell.execute_reply":"2022-04-13T18:09:43.422754Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: sympy in /opt/conda/lib/python3.7/site-packages (1.10)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.7/site-packages (from sympy) (1.2.1)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#  Tasks for Prospective GSoC 2022 Applicants for the \n#  Symbolic Calculation Project\n","metadata":{}},{"cell_type":"code","source":"import sympy as sp\nfrom sympy import simplify\nimport numpy as np\nfrom sympy.abc import x\nfrom sympy.parsing.sympy_parser import parse_expr\nfrom random import randint,choice\nfrom tqdm.notebook import trange, tqdm\nfrom torch.utils.data.dataset import Dataset\nfrom torch.utils.data import DataLoader\nimport os\nimport io\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport math\nimport time\nimport random\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"","metadata":{"id":"P3n7NOA1t-Yb","execution":{"iopub.status.busy":"2022-04-13T18:23:34.240513Z","iopub.execute_input":"2022-04-13T18:23:34.241190Z","iopub.status.idle":"2022-04-13T18:23:36.434289Z","shell.execute_reply.started":"2022-04-13T18:23:34.241082Z","shell.execute_reply":"2022-04-13T18:23:36.433567Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"These utility functions are take from original paper implementation of FAIR's [Deep Learning for Symbolic Mathematics](https://github.com/facebookresearch/SymbolicMathematics)  and some of them updated accordingly to fit this situation in hand","metadata":{}},{"cell_type":"markdown","source":"## Common Task 1. Dataset preprocessing  ","metadata":{}},{"cell_type":"markdown","source":"#### Utility\n","metadata":{"id":"kjLJAjYGVLLA"}},{"cell_type":"code","source":"OPERATORS = {\n    # Elementary functions\n    'add': 2,\n    'sub': 2,\n    'mul': 2,\n    'div': 2,\n    'pow': 2,\n    'rac': 2,\n    'inv': 1,\n    'pow':2,\n    'sqrt': 1,\n    'exp': 1,\n    'ln': 1,\n    # Trigonometric Functions\n    'sin': 1,\n    'cos': 1,\n    'tan': 1,\n    'cot': 1,\n    'sec': 1,\n    'csc': 1,\n    # Trigonometric Inverses\n}\n\nSYMPY_OPERATORS = {\n\n        # Elementary functions\n        sp.Add: 'add',\n        sp.Mul: 'mul',\n        sp.Pow: 'pow',\n        sp.exp: 'exp',\n        sp.log: 'ln',\n        # Trigonometric Functions\n        sp.sin: 'sin',\n        sp.cos: 'cos',\n        sp.tan: 'tan',\n        sp.cot: 'cot',\n        sp.sec: 'sec',\n        sp.csc: 'csc',\n\n    }\nvariable= {\n            'x': sp.Symbol('x', real=True), \n}\n\n\noperators = sorted(list(SYMPY_OPERATORS.values()))\nbase = 10\nbalanced = False\n\ndef _sympy_to_prefix(op, expr):\n    \"\"\"\n    Parse a SymPy expression given an initial root operator.\n    \"\"\"\n    n_args = len(expr.args)\n\n    assert (op == 'add' or op == 'mul') and (n_args >= 2) or (op != 'add' and op != 'mul') and (1 <= n_args <= 2)\n\n    # square root\n    if op == 'pow' and isinstance(expr.args[1], sp.Rational) and expr.args[1].p == 1 and expr.args[1].q == 2:\n        return ['sqrt'] + sympy_to_prefix(expr.args[0])\n\n    # parse children\n    parse_list = []\n    for i in range(n_args):\n        if i == 0 or i < n_args - 1:\n            parse_list.append(op)\n        parse_list += sympy_to_prefix(expr.args[i])\n\n    return parse_list\n\ndef write_int(val):\n    \"\"\"\n    Convert a decimal integer to a representation in the given base.\n    The base can be negative.\n    In balanced bases (positive), digits range from -(base-1)//2 to (base-1)//2\n    \"\"\"\n    base = 10\n    balanced = False\n    res = []\n    max_digit = abs(base)\n    if balanced:\n        max_digit = (base - 1) // 2\n    else:\n        if base > 0:\n            neg = val < 0\n            val = -val if neg else val\n    while True:\n        rem = val % base\n        val = val // base\n        if rem < 0 or rem > max_digit:\n            rem -= base\n            val += 1\n        res.append(str(rem))\n        if val == 0:\n            break\n    if base < 0 or balanced:\n        res.append('INT')\n    else:\n        res.append('INT-' if neg else 'INT+')\n    return res[::-1]\n\n\n\ndef sympy_to_prefix(expr):\n    \"\"\"\n    Convert a SymPy expression to a prefix one.\n    \"\"\"\n    if isinstance(expr, sp.Symbol):\n        return [str(expr)]\n    elif isinstance(expr, sp.Integer):\n        return write_int(int(str(expr)))\n    elif isinstance(expr, sp.Rational):\n        return ['div'] + write_int(int(expr.p)) + write_int(int(expr.q))\n    elif expr == sp.E:\n        return ['E']\n    elif expr == sp.pi:\n        return ['pi']\n    elif expr == sp.I:\n        return ['I']\n    # SymPy operator\n    for op_type, op_name in SYMPY_OPERATORS.items():\n        if isinstance(expr, op_type):\n            return _sympy_to_prefix(op_name, expr)\n    # unknown operator\n    raise Exception(f\"Unknown SymPy operator: {expr}\")\n\n\ndef parse_int(lst):\n    \"\"\"\n    Parse a list that starts with an integer.\n    Return the integer value, and the position it ends in the list.\n    \"\"\"\n    base = 10\n    balanced = False\n    print(lst)\n    val = 0\n    if not (balanced and lst[0] == 'INT' or base >= 2 and lst[0] in ['INT+', 'INT-'] or base <= -2 and lst[0] == 'INT'):\n        raise InvalidPrefixExpression(f\"Invalid integer in prefix expression\")\n    i = 0\n    for x in lst[1:]:\n        if not (x.isdigit() or x[0] == '-' and x[1:].isdigit()):\n            break\n        val = val * base + int(x)\n        i += 1\n    if base > 0 and lst[0] == 'INT-':\n        val = -val\n    return val, i + 1\n\ndef _prefix_to_infix(expr):\n    \"\"\"\n    Parse an expression in prefix mode, and output it in either:\n      - infix mode (returns human readable string)\n      - develop mode (returns a dictionary with the simplified expression)\n    \"\"\"\n    if len(expr) == 0:\n        raise InvalidPrefixExpression(\"Empty prefix list.\")\n    t = expr[0]\n    if t in operators:\n        args = []\n        l1 = expr[1:]\n        for _ in range(OPERATORS[t]):\n            i1, l1 = _prefix_to_infix(l1)\n            args.append(i1)\n        return write_infix(t, args), l1\n    elif t in variable :\n        return t, expr[1:]\n    else:\n        val, i = parse_int(expr)\n        return str(val), expr[i:]\n\ndef prefix_to_infix(expr):\n    \"\"\"\n    Prefix to infix conversion.\n    \"\"\"\n    p, r = _prefix_to_infix(expr)\n    if len(r) > 0:\n        raise InvalidPrefixExpression(f\"Incorrect prefix expression \\\"{expr}\\\". \\\"{r}\\\" was not parsed.\")\n    return f'({p})'\n\ndef write_infix(token, args):\n    \"\"\"\n    Infix representation.\n    Convert prefix expressions to a format that SymPy can parse.\n    \"\"\"\n    if token == 'add':\n        return f'({args[0]})+({args[1]})'\n    elif token == 'sub':\n        return f'({args[0]})-({args[1]})'\n    elif token == 'mul':\n        return f'({args[0]})*({args[1]})'\n    elif token == 'div':\n        return f'({args[0]})/({args[1]})'\n    elif token == 'pow':\n        return f'({args[0]})**({args[1]})'\n    elif token == 'rac':\n        return f'({args[0]})**(1/({args[1]}))'\n    elif token == 'abs':\n        return f'Abs({args[0]})'\n    elif token == 'inv':\n        return f'1/({args[0]})'\n    elif token in ['sign', 'sqrt', 'exp', 'ln', 'sin', 'cos', 'tan', 'cot', 'sec', 'csc', 'asin', 'acos', 'atan', 'acot', 'asec', 'acsc', 'sinh', 'cosh', 'tanh', 'coth', 'sech', 'csch', 'asinh', 'acosh', 'atanh', 'acoth', 'asech', 'acsch']:\n        return f'{token}({args[0]})'\n    elif token == 'derivative':\n        return f'Derivative({args[0]},{args[1]})'\n    elif token == 'f':\n        return f'f({args[0]})'\n    elif token == 'g':\n        return f'g({args[0]},{args[1]})'\n    elif token == 'h':\n        return f'h({args[0]},{args[1]},{args[2]})'\n    elif token.startswith('INT'):\n        return f'{token[-1]}{args[0]}'\n    else:\n        return token\n    raise InvalidPrefixExpression(f\"Unknown token in prefix expression: {token}, with arguments {args}\")\n\nwords = ['<s>','</s>'] + list(variable.keys()) +  operators + ['INT+', 'INT-', 'INT'] +[str(i) for i in range(10)]\nid2word = {i: s for i, s in enumerate(words)}\nword2id = {s: i for i, s in id2word.items()}","metadata":{"id":"_vfa76JQLtiw","execution":{"iopub.status.busy":"2022-04-13T18:23:37.585321Z","iopub.execute_input":"2022-04-13T18:23:37.585665Z","iopub.status.idle":"2022-04-13T18:23:37.641189Z","shell.execute_reply.started":"2022-04-13T18:23:37.585627Z","shell.execute_reply":"2022-04-13T18:23:37.640208Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# predict a sympy function\n\ndef pred_to_sympy(model , expr):\n  exp = []\n  for i in expr:\n    exp.append(sympy_to_prefix(i))\n  t = [torch.LongTensor([word2id[w] for w in pref if w in word2id]) for pref in exp]\n  lengths = torch.LongTensor([len(s) + 2 for s in t])\n  sent = torch.LongTensor(lengths.max().item(), lengths.size(0)).fill_(1)\n  assert lengths.min().item() > 2\n  sent[0] = 0\n  for i, s in enumerate(t):\n        sent[1:lengths[i] - 1, i].copy_(s)\n        sent[lengths[i] - 1, i] = 0\n  src = sent\n  return model.pred(src.to(device))\n","metadata":{"execution":{"iopub.status.busy":"2022-04-13T18:23:38.096830Z","iopub.execute_input":"2022-04-13T18:23:38.097069Z","iopub.status.idle":"2022-04-13T18:23:38.104182Z","shell.execute_reply.started":"2022-04-13T18:23:38.097041Z","shell.execute_reply":"2022-04-13T18:23:38.103518Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def idx_to_sp(idx, return_infix=False):\n    \"\"\"\n    Convert an indexed prefix expression to SymPy.\n    \"\"\"\n    prefix = [id2word[wid] for wid in idx]\n    infix = prefix_to_infix(prefix)\n    eq = sp.parse_expr(infix)\n    return (eq, infix) if return_infix else eq\n\ndef convert_to_text(batch,id2word):\n    \"\"\"\n    Convert a batch of sequences to a list of text sequences.\n    \"\"\"\n    batch = batch.cpu().numpy()\n    lengths = sum(batch != 1)\n\n    slen, bs = batch.shape\n    assert lengths.max() == slen-1 and lengths.shape[0] == bs\n    assert (batch == 0).sum() == bs\n    sequences = []\n\n    for j in range(bs):\n        words = []\n        for k in range(lengths[j]):\n            if batch[k, j] == 0:\n                break\n            words.append(id2word[batch[k, j]])\n        sequences.append(\" \".join(words))\n    return sequences","metadata":{"id":"lX3MpeutIfmI","execution":{"iopub.status.busy":"2022-04-13T18:23:38.719401Z","iopub.execute_input":"2022-04-13T18:23:38.720021Z","iopub.status.idle":"2022-04-13T18:23:38.728395Z","shell.execute_reply.started":"2022-04-13T18:23:38.719986Z","shell.execute_reply":"2022-04-13T18:23:38.727567Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"#### Generating Dataset","metadata":{"id":"qdYyDjsjVTyQ"}},{"cell_type":"markdown","source":"Here we have written the code for generating the Dataset\n\n### Algorithm\n1. Choose a Base Function  **fun** from {sin,cos,tan,exp,log,sec,cosec,cot}\n2. Generate a random Integer **i**  from {1,2,3} (It is the number of operators to combine)\n3. Iterate i times \n    *  Choose a combination operator **opr** from {+,*,/}\n    * Choose a random integer **k** from {1,2....10}\n    * Choose a Base Function  **fun_new**  from {sin,cos,tan,exp,log,sec,cosec,cot}\n    *  update **fun** <- **fun** **opr** k * **fun_new**\n4. retun **fun**","metadata":{}},{"cell_type":"code","source":"### Generating Functions in Sympy Format\n### Inspired from Using Algorithm 2 from https://ml4sci.org/assets/faseroh.pdf\n\nN = 100             #Number of functions to generate\ndebug = False       # For Debugging the Code\n\n\n# Combination and Base Functions\nCombination_Operator = [sp.Add , lambda x,y:x*sp.Pow(y , -1) , sp.Mul]\nbase_functions = [sp.exp,sp.log,sp.sin,sp.cos,sp.tan,sp.cot,sp.csc,sp.sec]\n\n\n# Opening three files with 'append' for train,validation and test split respectively\nf_train = open(\"data.train\", mode='a', encoding='utf-8')\nf_valid = open(\"data.valid\", mode='a', encoding='utf-8')\nf_test  = open(\"data.test\", mode='a', encoding='utf-8')\n\n# To generate 70% Train Data , 10% Validation Data and 20% Test Data\nsplit = [7,2,1] \n\nfor i in trange(N):\n  fun = choice(base_functions)(x) \n  if debug : print(f\"Initial function generated in Loop {i+1} is : {fun}\")\n  n = randint(1,3)  # Number of Operators to Combine\n  if debug : print(f\"Number of Operators Combine in Loop {i+1} is : {n}\")\n  for j in range(n-1):\n    Operator =  choice(Combination_Operator)\n    if debug : print(f\" Operators Combine Choosen in Loop {i+1} in {j+1} is {Operator}\")\n    fun = Operator(fun , randint(1,10)*choice(base_functions)(x))\n    \n    # Simplyfying Function\n\n  expr = simplify(fun)\n\n  # Creating a line to put in file with its series upto 4th order\n  line = (\" \".join(sympy_to_prefix(expr)) + \"\\t\" + \" \".join( sympy_to_prefix(expr.series(x,0,4).removeO())))\n  \n  if split[0] != 0:\n    f_train.write(line + \"\\n\")\n    split[0] = split[0] - 1\n  elif split[1] != 0:\n    f_test.write(line + \"\\n\")\n    split[1] = split[1] - 1\n  else:\n    f_valid.write(line + \"\\n\")\n    split = [7,2,1]\n  if debug : print(f\"Function : {sp.simplify(fun)} and Expansion : {sp.simplify(fun).series(x,0,4)}\")\n\nf_train.close()\nf_valid.close()\nf_test.close()\n","metadata":{"id":"1UIvoeXHPTpy","outputId":"c4284878-4f08-4426-ad97-cda853192ddf","execution":{"iopub.status.busy":"2022-04-13T18:23:40.431287Z","iopub.execute_input":"2022-04-13T18:23:40.431804Z","iopub.status.idle":"2022-04-13T18:23:53.144047Z","shell.execute_reply.started":"2022-04-13T18:23:40.431765Z","shell.execute_reply":"2022-04-13T18:23:53.143369Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/100 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9947a6c0f03d4731a5cbdbf7964782f6"}},"metadata":{}}]},{"cell_type":"markdown","source":"The above code is only for reproducibility , Actually I have ran this same code on my department server where it takes 2 days , on my computer it was not feasible to do this in so little time, I have attached the Gereated files too there are about 1800000 Training Samples","metadata":{}},{"cell_type":"markdown","source":"#### DataLoader\n","metadata":{"id":"L0hlY01ehoGa"}},{"cell_type":"code","source":"def collate_fn( elements):\n    \"\"\"\n    Collate samples into a batch.\n    \"\"\"\n    #print(elements)\n    x, y = zip(*elements)\n    #nb_ops = [sum(int(word in OPERATORS) for word in seq) for seq in x]\n    x = [torch.LongTensor([word2id[w] for w in seq if w in word2id]) for seq in x]\n    y = [torch.LongTensor([word2id[w] for w in seq if w in word2id]) for seq in y]\n    x = batch_sequences(x)\n    y = batch_sequences(y)\n    return x.to(device), y.to(device)\n\n\ndef batch_sequences(sequences):\n    \"\"\"\n    Take as input a list of n sequences (torch.LongTensor vectors) and return\n    a tensor of size (slen, n) where slen is the length of the longest\n    sentence, and a vector lengths containing the length of each sentence.\n    \"\"\"\n    #print(sequences)\n    lengths = torch.LongTensor([len(s) + 2 for s in sequences])\n    sent = torch.LongTensor(lengths.max().item(), lengths.size(0)).fill_(1)\n    assert lengths.min().item() > 2\n\n    sent[0] = 0\n    for i, s in enumerate(sequences):\n        sent[1:lengths[i] - 1, i].copy_(s)\n        sent[lengths[i] - 1, i] = 0\n\n    return sent","metadata":{"id":"lXKfJvv4h7Wf","execution":{"iopub.status.busy":"2022-04-13T18:23:53.145715Z","iopub.execute_input":"2022-04-13T18:23:53.146108Z","iopub.status.idle":"2022-04-13T18:23:53.156646Z","shell.execute_reply.started":"2022-04-13T18:23:53.146070Z","shell.execute_reply":"2022-04-13T18:23:53.155909Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class Dataset_Loader(Dataset):\n  r\"\"\"PyTorch Dataset class for loading data.\n\n  This is where the data parsing happens.\n\n  This class is built with reusability in mind.\n\n  Arguments:\n\n    path (:obj:`str`):\n        Path to the data partition.\n\n  \"\"\"\n\n  def __init__(self, path):\n\n    # Check if path exists.\n    if not os.path.isfile(path):\n      # Raise error if path is invalid.\n      raise ValueError('Invalid `path` variable! Needs to be a directory')\n\n    self.SRC = []\n    self.TRG = []\n    # Since the labels are defined by folders with data we loop\n    # through each label.\n\n\n\n    with open(path, mode='r', encoding='utf-8') as f:\n          lines = [line.rstrip() for line in f]\n          self.data = [xy.split('\\t') for  xy in lines]\n          self.data = [xy for xy in self.data if len(xy) == 2]\n    for src,trg in self.data:\n      self.SRC.append(src)\n      self.TRG.append(trg)\n\n    # Number of examples.\n    self.n_examples = len(self.SRC)\n\n    return\n\n\n  def __len__(self):\n    r\"\"\"When used `len` return the number of examples.\n\n    \"\"\"\n\n    return self.n_examples\n\n\n  def __getitem__(self, item):\n    r\"\"\"Given an index return an example from the position.\n\n    Arguments:\n\n      item (:obj:`int`):\n          Index position to pick an example to return.\n\n    Returns:\n      :obj:`Dict[str, str]`: Dictionary of inputs that are used to feed\n      to a model.\n\n    \"\"\"\n\n    return self.SRC[item].split(),self.TRG[item].split()\n    ","metadata":{"id":"Ap6Dv1Gp2YbP","execution":{"iopub.status.busy":"2022-04-13T18:23:53.159423Z","iopub.execute_input":"2022-04-13T18:23:53.159656Z","iopub.status.idle":"2022-04-13T18:23:53.171283Z","shell.execute_reply.started":"2022-04-13T18:23:53.159626Z","shell.execute_reply":"2022-04-13T18:23:53.170529Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"train_data_loader = Dataset_Loader('data.train')\nvalid_data_loader = Dataset_Loader('data.valid')","metadata":{"id":"6BcxNL6Dr32x","_kg_hide-output":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_iterator = DataLoader(\n    train_data_loader,\n    batch_size=256,\n    shuffle=True,\n    num_workers=0,\n    collate_fn=collate_fn,\n    pin_memory=False,\n )\nvalid_iterator = DataLoader(\n    atc,\n    batch_size=256,\n    shuffle=True,\n    num_workers=0,\n    collate_fn=collate_fn,\n    pin_memory=False,\n )","metadata":{"id":"e7_ssxdoD6X6","execution":{"iopub.status.busy":"2022-04-07T09:02:38.338884Z","iopub.execute_input":"2022-04-07T09:02:38.339171Z","iopub.status.idle":"2022-04-07T09:02:38.346309Z","shell.execute_reply.started":"2022-04-07T09:02:38.339136Z","shell.execute_reply":"2022-04-07T09:02:38.345416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Common Task 2. Use LSTM model","metadata":{}},{"cell_type":"markdown","source":"### LSTM Model","metadata":{"id":"Pq05a13UWex_"}},{"cell_type":"code","source":"SEED = 1234\n\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True","metadata":{"id":"x3UQ9nQyQ_Ku","execution":{"iopub.status.busy":"2022-04-13T18:24:10.652333Z","iopub.execute_input":"2022-04-13T18:24:10.652655Z","iopub.status.idle":"2022-04-13T18:24:10.661020Z","shell.execute_reply.started":"2022-04-13T18:24:10.652616Z","shell.execute_reply":"2022-04-13T18:24:10.660280Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### Encoder","metadata":{}},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n        super().__init__()\n        \n        self.hid_dim = hid_dim\n        self.n_layers = n_layers\n        self.embedding = nn.Embedding(input_dim, emb_dim)\n        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, src):\n        embedded = self.dropout(self.embedding(src))\n        outputs, (hidden, cell) = self.rnn(embedded)\n        return hidden, cell","metadata":{"id":"TKM-zx2NodLL","execution":{"iopub.status.busy":"2022-04-13T18:24:11.432003Z","iopub.execute_input":"2022-04-13T18:24:11.432545Z","iopub.status.idle":"2022-04-13T18:24:11.439860Z","shell.execute_reply.started":"2022-04-13T18:24:11.432501Z","shell.execute_reply":"2022-04-13T18:24:11.438962Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"### Decoder","metadata":{}},{"cell_type":"code","source":"class Decoder(nn.Module):\n    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n        super().__init__()\n        \n        self.output_dim = output_dim\n        self.hid_dim = hid_dim\n        self.n_layers = n_layers\n        self.embedding = nn.Embedding(output_dim, emb_dim)\n        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n        self.fc_out = nn.Linear(hid_dim, output_dim)\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, input, hidden, cell):\n\n        input = input.unsqueeze(0)\n        embedded = self.dropout(self.embedding(input))    \n        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n        prediction = self.fc_out(output.squeeze(0))\n        return prediction, hidden, cell","metadata":{"id":"Q19T8iLrova7","execution":{"iopub.status.busy":"2022-04-13T18:24:12.448584Z","iopub.execute_input":"2022-04-13T18:24:12.448842Z","iopub.status.idle":"2022-04-13T18:24:12.456413Z","shell.execute_reply.started":"2022-04-13T18:24:12.448808Z","shell.execute_reply":"2022-04-13T18:24:12.455433Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"### seq2seq Model","metadata":{}},{"cell_type":"code","source":"class Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder, device):\n        super().__init__()\n        \n        self.encoder = encoder\n        self.decoder = decoder\n        self.device = device\n        \n        assert encoder.hid_dim == decoder.hid_dim, \\\n            \"Hidden dimensions of encoder and decoder must be equal!\"\n        assert encoder.n_layers == decoder.n_layers, \\\n            \"Encoder and decoder must have equal number of layers!\"\n        \n    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n        \n        batch_size = trg.shape[1]\n        trg_len = trg.shape[0]\n        trg_vocab_size = self.decoder.output_dim\n        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n        hidden, cell = self.encoder(src)\n        input = trg[0,:]\n        \n        for t in range(1, trg_len):\n            output, hidden, cell = self.decoder(input, hidden, cell)\n            outputs[t] = output\n            teacher_force = random.random() < teacher_forcing_ratio\n            top1 = output.argmax(1) \n            input = trg[t] if teacher_force else top1\n        return outputs\n\n    def pred(self,src):\n        input = src[0,:].to(device)\n        hidden, cell = self.encoder(src)\n        output, hidden, cell = self.decoder(input, hidden, cell)\n        result = output.argmax(1)\n        cram = result == 0\n        lst = []\n        lst.append(result.tolist())\n        for i in range(100):\n              output, hidden, cell = self.decoder(result, hidden, cell)\n              result = output.argmax(1)\n              result[cram] = 1\n              cram[result == 0] = True\n              lst.append(result.tolist())\n              if all(result == 1) : break\n\n        return torch.tensor(lst)","metadata":{"id":"IdQBDUHQo0UK","execution":{"iopub.status.busy":"2022-04-13T18:24:13.846746Z","iopub.execute_input":"2022-04-13T18:24:13.847571Z","iopub.status.idle":"2022-04-13T18:24:13.869439Z","shell.execute_reply.started":"2022-04-13T18:24:13.847454Z","shell.execute_reply":"2022-04-13T18:24:13.868522Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"### HyperParameters","metadata":{}},{"cell_type":"code","source":"INPUT_DIM = len(word2id)\nOUTPUT_DIM = len(word2id)\nENC_EMB_DIM = 256\nDEC_EMB_DIM = 256\nHID_DIM = 512\nN_LAYERS = 2\nENC_DROPOUT = 0.2\nDEC_DROPOUT = 0.2\n\nenc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\ndec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n\nmodel = Seq2Seq(enc, dec, device).to(device)","metadata":{"id":"2HE-lRoBo66a","execution":{"iopub.status.busy":"2022-04-13T18:24:15.965105Z","iopub.execute_input":"2022-04-13T18:24:15.965675Z","iopub.status.idle":"2022-04-13T18:24:19.349033Z","shell.execute_reply.started":"2022-04-13T18:24:15.965635Z","shell.execute_reply":"2022-04-13T18:24:19.348299Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def init_weights(m):\n    for name, param in m.named_parameters():\n        nn.init.uniform_(param.data, -0.08, 0.08)\n        \nmodel.apply(init_weights)","metadata":{"id":"I2z0uQ7jtcQd","outputId":"a31884e1-fac0-44e5-cc38-74f9485d723e","execution":{"iopub.status.busy":"2022-04-13T18:24:19.350461Z","iopub.execute_input":"2022-04-13T18:24:19.350695Z","iopub.status.idle":"2022-04-13T18:24:19.361680Z","shell.execute_reply.started":"2022-04-13T18:24:19.350662Z","shell.execute_reply":"2022-04-13T18:24:19.360831Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"Seq2Seq(\n  (encoder): Encoder(\n    (embedding): Embedding(27, 256)\n    (rnn): LSTM(256, 512, num_layers=2, dropout=0.2)\n    (dropout): Dropout(p=0.2, inplace=False)\n  )\n  (decoder): Decoder(\n    (embedding): Embedding(27, 256)\n    (rnn): LSTM(256, 512, num_layers=2, dropout=0.2)\n    (fc_out): Linear(in_features=512, out_features=27, bias=True)\n    (dropout): Dropout(p=0.2, inplace=False)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"def count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f'The model has {count_parameters(model):,} trainable parameters')","metadata":{"id":"nwWTUDN8tlH1","outputId":"acd13d6d-9683-4bd8-cb5f-dfec16e31ad5","execution":{"iopub.status.busy":"2022-04-13T18:24:25.552850Z","iopub.execute_input":"2022-04-13T18:24:25.553105Z","iopub.status.idle":"2022-04-13T18:24:25.558959Z","shell.execute_reply.started":"2022-04-13T18:24:25.553076Z","shell.execute_reply":"2022-04-13T18:24:25.558060Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"The model has 7,384,091 trainable parameters\n","output_type":"stream"}]},{"cell_type":"code","source":"optimizer = optim.Adam(model.parameters())","metadata":{"id":"fSMeqCaWtvdZ","execution":{"iopub.status.busy":"2022-04-13T18:24:27.649310Z","iopub.execute_input":"2022-04-13T18:24:27.649828Z","iopub.status.idle":"2022-04-13T18:24:27.654016Z","shell.execute_reply.started":"2022-04-13T18:24:27.649789Z","shell.execute_reply":"2022-04-13T18:24:27.653055Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"TRG_PAD_IDX = 1\n\ncriterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)","metadata":{"id":"KIH6Ja2MJ0Pf","execution":{"iopub.status.busy":"2022-04-13T18:24:32.017233Z","iopub.execute_input":"2022-04-13T18:24:32.017764Z","iopub.status.idle":"2022-04-13T18:24:32.021985Z","shell.execute_reply.started":"2022-04-13T18:24:32.017725Z","shell.execute_reply":"2022-04-13T18:24:32.020779Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def train(model, iterator, optimizer, criterion, clip):\n    \n    model.train()\n    \n    epoch_loss = 0\n    \n    for i, batch in enumerate(iterator):\n        \n        src = batch[0]\n        trg = batch[1]\n        if debug : print(f\"Batch {i} have input dimension {src.shape} and output dimension {trg.shape}\")\n        optimizer.zero_grad()\n        \n        output = model(src, trg)\n        \n        #trg = [trg len, batch size]\n        #output = [trg len, batch size, output dim]\n        \n        output_dim = output.shape[-1]\n        \n        output = output[1:].view(-1, output_dim)\n        trg = trg[1:].view(-1)\n        \n        #trg = [(trg len - 1) * batch size]\n        #output = [(trg len - 1) * batch size, output dim]\n        \n        loss = criterion(output, trg)\n        \n        loss.backward()\n        \n        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n        \n        optimizer.step()\n        \n        epoch_loss += loss.item()\n        \n    return epoch_loss / len(iterator)","metadata":{"id":"9IkVRu2Ht0tw","execution":{"iopub.status.busy":"2022-04-13T18:25:39.844217Z","iopub.execute_input":"2022-04-13T18:25:39.844812Z","iopub.status.idle":"2022-04-13T18:25:39.861817Z","shell.execute_reply.started":"2022-04-13T18:25:39.844773Z","shell.execute_reply":"2022-04-13T18:25:39.860809Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"def evaluate(model, iterator, criterion):\n    \n    model.eval()\n    \n    epoch_loss = 0\n    \n    with torch.no_grad():\n    \n        for i, batch in enumerate(iterator):\n\n            src = batch[0]\n            trg = batch[1]\n\n            output = model(src, trg, 0) #turn off teacher forcing\n\n            #trg = [trg len, batch size]\n            #output = [trg len, batch size, output dim]\n\n            output_dim = output.shape[-1]\n            \n            output = output[1:].view(-1, output_dim)\n            trg = trg[1:].view(-1)\n\n            #trg = [(trg len - 1) * batch size]\n            #output = [(trg len - 1) * batch size, output dim]\n\n            loss = criterion(output, trg)\n            \n            epoch_loss += loss.item()\n        \n    return epoch_loss / len(iterator)","metadata":{"id":"NM-6Bmr1vRRX","execution":{"iopub.status.busy":"2022-04-13T18:25:40.886933Z","iopub.execute_input":"2022-04-13T18:25:40.887719Z","iopub.status.idle":"2022-04-13T18:25:40.894493Z","shell.execute_reply.started":"2022-04-13T18:25:40.887677Z","shell.execute_reply":"2022-04-13T18:25:40.893785Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"def epoch_time(start_time, end_time):\n    elapsed_time = end_time - start_time\n    elapsed_mins = int(elapsed_time / 60)\n    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n    return elapsed_mins, elapsed_secs","metadata":{"id":"AMeIhE-MvVs2","execution":{"iopub.status.busy":"2022-04-13T18:25:44.677901Z","iopub.execute_input":"2022-04-13T18:25:44.678431Z","iopub.status.idle":"2022-04-13T18:25:44.683231Z","shell.execute_reply.started":"2022-04-13T18:25:44.678389Z","shell.execute_reply":"2022-04-13T18:25:44.682042Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"N_EPOCHS = 30\nCLIP = 1\n\nbest_valid_loss = float('inf')\n\nfor epoch in range(N_EPOCHS):\n    \n    start_time = time.time()\n    \n    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n    valid_loss = evaluate(model, valid_iterator, criterion)\n    \n    end_time = time.time()\n    \n    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n    \n    if valid_loss < best_valid_loss:\n        best_valid_loss = valid_loss\n        torch.save(model.state_dict(), 'tut1-model.pt')\n    \n    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')","metadata":{"id":"kQvYpN_wvWUP","outputId":"491fa098-d07d-4fed-92de-c084c5a60198","execution":{"iopub.status.busy":"2022-04-07T03:32:03.204852Z","iopub.execute_input":"2022-04-07T03:32:03.205442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I have runned the Model Several times and saved the model as tut1-mode (3).pt , We can call the model from following codel","metadata":{}},{"cell_type":"code","source":"model.load_state_dict(torch.load('../input/modelpy1/tut1-model (3).pt'))","metadata":{"execution":{"iopub.status.busy":"2022-04-13T18:25:15.024237Z","iopub.execute_input":"2022-04-13T18:25:15.025071Z","iopub.status.idle":"2022-04-13T18:25:15.331964Z","shell.execute_reply.started":"2022-04-13T18:25:15.025018Z","shell.execute_reply":"2022-04-13T18:25:15.331264Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"markdown","source":"### Testing","metadata":{}},{"cell_type":"code","source":"test_loader = Dataset_Loader('../input/testingdata/data.test')","metadata":{"execution":{"iopub.status.busy":"2022-04-13T18:25:21.917636Z","iopub.execute_input":"2022-04-13T18:25:21.918374Z","iopub.status.idle":"2022-04-13T18:25:23.371083Z","shell.execute_reply.started":"2022-04-13T18:25:21.918336Z","shell.execute_reply":"2022-04-13T18:25:23.370315Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"test_iterator = DataLoader(\n    test_loader,\n    batch_size=256,\n    shuffle=True,\n    num_workers=0,\n    collate_fn=collate_fn,\n    pin_memory=False,\n )","metadata":{"execution":{"iopub.status.busy":"2022-04-13T18:25:25.773450Z","iopub.execute_input":"2022-04-13T18:25:25.773693Z","iopub.status.idle":"2022-04-13T18:25:25.778420Z","shell.execute_reply.started":"2022-04-13T18:25:25.773664Z","shell.execute_reply":"2022-04-13T18:25:25.777398Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"test_loss = evaluate(model,test_iterator, criterion)","metadata":{"execution":{"iopub.status.busy":"2022-04-13T18:25:52.629721Z","iopub.execute_input":"2022-04-13T18:25:52.629983Z","iopub.status.idle":"2022-04-13T18:27:23.139088Z","shell.execute_reply.started":"2022-04-13T18:25:52.629954Z","shell.execute_reply":"2022-04-13T18:27:23.138327Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"print(f\"Test Loss: {test_loss}\" )","metadata":{"execution":{"iopub.status.busy":"2022-04-13T18:27:23.140638Z","iopub.execute_input":"2022-04-13T18:27:23.140860Z","iopub.status.idle":"2022-04-13T18:27:23.145627Z","shell.execute_reply.started":"2022-04-13T18:27:23.140828Z","shell.execute_reply":"2022-04-13T18:27:23.144934Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Test Loss: 0.05521275207651698\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}